{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f05e17",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a510213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests as req\n",
    "import urllib.request\n",
    "\n",
    "import chromadb\n",
    "\n",
    "import openai\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from llama_index.core import Document, Settings, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.nomic import NomicEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from utils_15B import extract_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87579f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.7\n",
      "IPython version      : 8.20.0\n",
      "\n",
      "llama_index.core: 0.10.12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p llama_index.core\n",
    "\n",
    "# Python implementation: CPython\n",
    "# Python version       : 3.11.7\n",
    "# IPython version      : 8.20.0\n",
    "\n",
    "# llama_index.core: 0.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc182fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_cpp_python                         0.2.53\r\n",
      "llama-index-core                         0.10.12\r\n",
      "llama-index-embeddings-huggingface       0.1.4\r\n",
      "llama-index-embeddings-nomic             0.1.6\r\n",
      "llama-index-embeddings-openai            0.1.6\r\n",
      "llama-index-llms-huggingface             0.1.3\r\n",
      "llama-index-llms-llama-cpp               0.1.3\r\n",
      "llama-index-llms-openai                  0.1.6\r\n",
      "llama-index-vector-stores-chroma         0.0.1\r\n",
      "llamaindex-py-client                     0.1.13\r\n",
      "loguru                                   0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep ^l\n",
    "\n",
    "# llama_cpp_python                         0.2.53\n",
    "# llama-index-core                         0.10.12\n",
    "# llama-index-embeddings-openai            0.1.6\n",
    "# llama-index-llms-llama-cpp               0.1.3\n",
    "# llama-index-llms-openai                  0.1.6\n",
    "# llama-index-vector-stores-chroma         0.0.1\n",
    "# llamaindex-py-client                     0.1.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b27c92",
   "metadata": {},
   "source": [
    "## Verify API tokens are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44ccd20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T15:27:26.245184400Z",
     "start_time": "2024-02-22T15:27:26.230187Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()  # This loads the variables from .envz\n",
    "nomic_api_key = os.getenv(\"NOMIC_API_KEY\")\n",
    "# print(nomic_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9d925",
   "metadata": {},
   "source": [
    "## (Optional) Remove previous JSON files and Chroma DB before starting\n",
    "<span style=\"color: darkred; font-size: 18px;\">using macOS/Linux %%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39a8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa45741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ./ -type f -name \"*.json\" -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30698a6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fa4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GGUF (GGML deprecated)\n",
    "# format example for download from https://huggingface.co/TheBloke\n",
    "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf\n",
    "\n",
    "def download_file(the_bloke_link, model_name):\n",
    "    # Checks if the file already exists before downloading\n",
    "    if not os.path.isfile(model_name):\n",
    "        urllib.request.urlretrieve(the_bloke_link, model_name)\n",
    "        print(f\"model {model_name}  is available, downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"model: {model_name}  is available, previously downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351e42c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: llama-2-7b-chat.Q4_0.gguf  is available, previously downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Dowloading GGUF model from https://huggingface.co/TheBloke\n",
    "\n",
    "download_link = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf\"\n",
    "model_name = download_link .split(\"/\")[-1]\n",
    "# print(f\"model: {model_name}\")\n",
    "\n",
    "download_file(download_link, model_name)\n",
    "model_path = f\"./{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c21d4",
   "metadata": {},
   "source": [
    "###  Llama 2 chat prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ed1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823c59e",
   "metadata": {},
   "source": [
    "### Llama 2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54387f05",
   "metadata": {},
   "source": [
    "#### Dmitry's CPP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e18e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = LlamaCPP(\n",
    "#     # You can pass in the URL to a GGML model to download it automatically\n",
    "#     model_url=model_url,\n",
    "#     # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "#     model_path=None,\n",
    "#     temperature=0.1,\n",
    "#     max_new_tokens=512,\n",
    "#     # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "#     context_window=3900,\n",
    "#     # kwargs to pass to __call__()\n",
    "#     generate_kwargs={},\n",
    "#     # kwargs to pass to __init__()\n",
    "#     # set to at least 1 to use GPU\n",
    "#     # model_kwargs={\"n_gpu_layers\": 24},\n",
    "#     #messages_to_prompt=messages_to_prompt,\n",
    "#     #completion_to_prompt=completion_to_prompt,\n",
    "#     #verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b955bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex reference: https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoLlama-Local.html\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     context_window=4096,\n",
    "#     max_new_tokens=2048,\n",
    "#     generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "#     query_wrapper_prompt=query_wrapper_prompt,\n",
    "#     tokenizer_name=selected_model,\n",
    "#     model_name=selected_model,\n",
    "#     device_map=\"auto\",\n",
    "#     # change these settings below depending on your GPU\n",
    "#     model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c1de1",
   "metadata": {},
   "source": [
    "### using LlamaIndex reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d01298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = model_name\n",
    "# selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3149840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    context_window=4096,\n",
    "    max_new_tokens=2048,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=selected_model,\n",
    "    model_name=selected_model,\n",
    "    device_map=\"auto\",\n",
    "    # change these settings below depending on your GPU\n",
    "#     model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
    "    model_kwargs={},  # NO GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94b50b",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed015a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this as baseline standard\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f4eb4",
   "metadata": {},
   "source": [
    "## Fetch data from \"specific\" clinicaltrials.gov\n",
    "<span style=\"color: darkred; font-size: 18px;\"> source: https://drive.google.com/file/d/1HOsN3v8DLzwoMOXOr_Mfb1Hn6XNwlZ72/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b83137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial(nct_id):\n",
    "    trial = req.get(f\"https://clinicaltrials.gov/api/v2/studies/{nct_id}\")\n",
    "    trial_json = trial.json()\n",
    "    return trial_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231c678c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T15:27:27.632177500Z",
     "start_time": "2024-02-22T15:27:27.430175800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some trials to consider (interventional, completed):\n",
    "# nct_id = \"NCT00094887\"\n",
    "# nct_id = \"NCT00108953\"\n",
    "# nct_id = \"NCT00177671\" \n",
    "# nct_id = \"NCT00281918\"\n",
    "# nct_id = \"NCT00404079\"\n",
    "# nct_id = \"NCT00426751\"\n",
    "# nct_id = \"NCT01865747\" #<== good one \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14f90f",
   "metadata": {},
   "source": [
    "### Use just one one trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caef252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_nct_id = [\n",
    "#     \"NCT00094887\",\n",
    "#     \"NCT00108953\",\n",
    "#     \"NCT00177671\",\n",
    "#     \"NCT00281918\",\n",
    "#     \"NCT00404079\",\n",
    "#     \"NCT00426751\",\n",
    "#     \"NCT01865747\",\n",
    "# ]\n",
    "\n",
    "list_of_nct_id = [\n",
    "    \"NCT00108953\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6122ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downloaded_json(list_of_nct_id):\n",
    "    downloaded_json = []\n",
    "    for nct_id in list_of_nct_id:\n",
    "        trial = get_trial(nct_id)\n",
    "        downloaded_json.append(trial)\n",
    "        # save locally for reference\n",
    "        with open(f\"{nct_id}.json\", \"w\") as f:\n",
    "            json.dump(trial, f, indent=4)\n",
    "    return downloaded_json\n",
    "\n",
    "downloaded_json = get_downloaded_json(list_of_nct_id)\n",
    "# downloaded_json[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c098cf2",
   "metadata": {},
   "source": [
    "## Extract a  subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e06d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_from_extracted_json(downloaded_json):    \n",
    "    documents_list  = []\n",
    "    for json_file in downloaded_json:\n",
    "        extracted_json = extract_from_json(json_file)\n",
    "        nct_id = json_file['protocolSection']['identificationModule']['nctId']\n",
    "        # save manipulated JSON file to disk for review\n",
    "        save_path = f\"{nct_id}_extracted.json\"\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(extracted_json, f, indent=4)\n",
    "        # prepare for indexing\n",
    "        documents_list.append(extracted_json)\n",
    "    return documents_list\n",
    "    \n",
    "documents_list = list_from_extracted_json(downloaded_json)\n",
    "# len(documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f63e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_list[0].keys() # useful later to adjust metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5d6ba",
   "metadata": {},
   "source": [
    "## Llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef51c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c0507",
   "metadata": {},
   "source": [
    "### add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4005a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the keys (for metadata)\n",
    "all_keys = list(documents_list[0].keys())\n",
    "# all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22c01b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to adjust the metadata keys used\n",
    "llm_keys_to_incude = [\n",
    "    \"Brief title\",\n",
    "    \"National Clinical Identification NCT ID\",\n",
    "    \"Lead sponsor\",\n",
    "    \"Arms group 0 intervention names\",\n",
    "    \"Enrollment count\",\n",
    "]\n",
    "\n",
    "llm_keys_to_exclude = [key for key in all_keys if key not in llm_keys_to_incude]\n",
    "\n",
    "# for simplicity, do the same for embedding_keys_to_exclude (in this example)\n",
    "embedding_keys_to_exclude = llm_keys_to_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfe96e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:  metata data must be one of (str, int, float, None)\n",
    "# use json.dumps() to convert lists and dictionaries into strings\n",
    "\n",
    "def create_llama_docs(documents_list):\n",
    "    llama_documents = []\n",
    "\n",
    "    for trial in documents_list:\n",
    "        trial[\"Brief title\"] = json.dumps(trial[\"Brief title\"])\n",
    "        trial[\"Official title\"] = json.dumps(trial[\"Official title\"])\n",
    "        trial[\"Brief summary\"] = json.dumps(trial[\"Brief summary\"])\n",
    "        trial[\"Detailed description\"] = json.dumps(trial[\"Detailed description\"])\n",
    "        trial[\"Arms group 0 intervention name\"] = json.dumps(trial[\"Arms group 0 intervention name\"])\n",
    "        trial[\"Arms group 1 intervention name\"] = json.dumps(trial[\"Arms group 1 intervention name\"])\n",
    "        trial[\"Eligibility minimum age\"] = json.dumps(trial[\"Eligibility minimum age\"])\n",
    "        trial[\"Organization\"] = json.dumps(trial[\"Organization\"])\n",
    "\n",
    "        # create a Llama Document object \n",
    "        # with text and excluded meta data for llm and embedding model\n",
    "        llama_document = Document(\n",
    "            text=trial[\"Detailed description\"],\n",
    "#             text=json.dumps(trial), #<== testing\n",
    "            metadata=trial,\n",
    "            excluded_llm_metadata_keys=llm_keys_to_exclude,\n",
    "            excluded_embed_metadata_keys=embedding_keys_to_exclude ,\n",
    "            metadata_template=\"{key}=>{value}\",\n",
    "            text_template=\"Metadata:\\n{metadata_str}\\n===========================\\nContent: \\n{content}\"\n",
    "        )\n",
    "        llama_documents.append(llama_document)\n",
    "    \n",
    "    return llama_documents\n",
    "\n",
    "llama_documents = create_llama_docs(documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adf15760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      "National Clinical Identification NCT ID=>NCT00108953\n",
      "Brief title=>\"A Research Study to Treat Patients With Advanced Hepatocellular Carcinoma\"\n",
      "Lead sponsor=>Bayer\n",
      "Enrollment count=>96\n",
      "Arms group 0 intervention names=>['Drug: Sorafenib (Nexavar, BAY43-9006) plus Doxorubicin']\n",
      "===========================\n",
      "Content: \n",
      "\"In addition to the key secondary outcome parameters the following parameters will be assessed in an exploratory manner: relative time to progression (TTP), time to symptomatic progression (TTSP), response rate (RR) and overall survival between the 2 study populations.\\n\\nThe possible and potential predictive assays of clinical benefit through an assessment of the correlation between the defined baseline characteristics and key clinical endpoints.\\n\\nThe safety and tolerability will be assessed in the adverse event section. Doxorubicin pharmacokinetics in HCC patients treated with sorafenib versus placebo will be compared and the pharmacokinetic data will be correlated with doxorubicin-related adverse events (i.e., cardiotoxicity).\"\n"
     ]
    }
   ],
   "source": [
    "# Example —LLM sees this:\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab6baf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      "National Clinical Identification NCT ID=>NCT00108953\n",
      "Brief title=>\"A Research Study to Treat Patients With Advanced Hepatocellular Carcinoma\"\n",
      "Lead sponsor=>Bayer\n",
      "Enrollment count=>96\n",
      "Arms group 0 intervention names=>['Drug: Sorafenib (Nexavar, BAY43-9006) plus Doxorubicin']\n",
      "===========================\n",
      "Content: \n",
      "\"In addition to the key secondary outcome parameters the following parameters will be assessed in an exploratory manner: relative time to progression (TTP), time to symptomatic progression (TTSP), response rate (RR) and overall survival between the 2 study populations.\\n\\nThe possible and potential predictive assays of clinical benefit through an assessment of the correlation between the defined baseline characteristics and key clinical endpoints.\\n\\nThe safety and tolerability will be assessed in the adverse event section. Doxorubicin pharmacokinetics in HCC patients treated with sorafenib versus placebo will be compared and the pharmacokinetic data will be correlated with doxorubicin-related adverse events (i.e., cardiotoxicity).\"\n"
     ]
    }
   ],
   "source": [
    "# Example — Embedding model sees this:\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5ad18",
   "metadata": {},
   "source": [
    "## Create Nodes\n",
    "<span style=\"color: darkred; font-size: 15px;\">adjust chunk_size, chunk_overlap</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13c22a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_nodes(llama_documents):\n",
    "    parser = SentenceSplitter(chunk_size=2048,chunk_overlap=40) # <== adjust(default = 1024/20)\n",
    "    nodes = parser.get_nodes_from_documents(llama_documents)\n",
    "\n",
    "    for node in nodes:\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=MetadataMode.EMBED)\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    return nodes\n",
    "\n",
    "nodes = create_nodes(llama_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0e579",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8c6826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for the CLINICAL_RAG collection in the database...\n",
      "CLINICAL_RAG collection WAS NOT FOUND in Chroma DB, creating...\n",
      "Creating vector store...\n",
      "Creating vector store index\n",
      "record count: 1\n"
     ]
    }
   ],
   "source": [
    "# Chroma DB collection name\n",
    "COLLECTION_NAME = \"CLINICAL_RAG\"\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "print(f\"Looking for the {COLLECTION_NAME} collection in the database...\" )\n",
    "if COLLECTION_NAME not in [col.name for col in db.list_collections()]:\n",
    "    print(f\"{COLLECTION_NAME} collection WAS NOT FOUND in Chroma DB, creating...\")\n",
    "    chroma_collection = db.create_collection(COLLECTION_NAME)\n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    print(\"Creating vector store index\")\n",
    "    VectorStoreIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        store_nodes_override=True\n",
    "    )\n",
    "    print(f\"record count: {chroma_collection.count()}\"     \n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(f\"{COLLECTION_NAME} collection WAS FOUND in Chroma DB\")\n",
    "    COLLECTION_NAME = db.get_collection(COLLECTION_NAME)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=COLLECTION_NAME)\n",
    "    print(\"Restoring vector store index from the collection...\")\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model,\n",
    "        store_nodes_override=True\n",
    "    )\n",
    "\n",
    "    print(f\"record count: {COLLECTION_NAME.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce45d1",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e732d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fd7f7",
   "metadata": {},
   "source": [
    "### Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e706e81",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'context_window'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine()\n",
      "File \u001b[0;32m~/anaconda3/envs/cap_h/lib/python3.11/site-packages/llama_index/core/indices/base.py:391\u001b[0m, in \u001b[0;36mBaseIndex.as_query_engine\u001b[0;34m(self, llm, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_retriever(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    385\u001b[0m llm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    386\u001b[0m     resolve_llm(llm, callback_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m llm_from_settings_or_context(Settings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_context)\n\u001b[1;32m    389\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RetrieverQueryEngine\u001b[38;5;241m.\u001b[39mfrom_args(\n\u001b[1;32m    392\u001b[0m     retriever,\n\u001b[1;32m    393\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    395\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cap_h/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py:108\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.from_args\u001b[0;34m(cls, retriever, llm, response_synthesizer, node_postprocessors, response_mode, text_qa_template, refine_template, summary_template, simple_template, output_cls, use_async, streaming, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a RetrieverQueryEngine object.\".\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[0;32m--> 108\u001b[0m response_synthesizer \u001b[38;5;241m=\u001b[39m response_synthesizer \u001b[38;5;129;01mor\u001b[39;00m get_response_synthesizer(\n\u001b[1;32m    109\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    110\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[1;32m    111\u001b[0m     text_qa_template\u001b[38;5;241m=\u001b[39mtext_qa_template,\n\u001b[1;32m    112\u001b[0m     refine_template\u001b[38;5;241m=\u001b[39mrefine_template,\n\u001b[1;32m    113\u001b[0m     summary_template\u001b[38;5;241m=\u001b[39msummary_template,\n\u001b[1;32m    114\u001b[0m     simple_template\u001b[38;5;241m=\u001b[39msimple_template,\n\u001b[1;32m    115\u001b[0m     response_mode\u001b[38;5;241m=\u001b[39mresponse_mode,\n\u001b[1;32m    116\u001b[0m     output_cls\u001b[38;5;241m=\u001b[39moutput_cls,\n\u001b[1;32m    117\u001b[0m     use_async\u001b[38;5;241m=\u001b[39muse_async,\n\u001b[1;32m    118\u001b[0m     streaming\u001b[38;5;241m=\u001b[39mstreaming,\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    121\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager_from_settings_or_context(\n\u001b[1;32m    122\u001b[0m     Settings, service_context\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    126\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m    127\u001b[0m     response_synthesizer\u001b[38;5;241m=\u001b[39mresponse_synthesizer,\n\u001b[1;32m    128\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    129\u001b[0m     node_postprocessors\u001b[38;5;241m=\u001b[39mnode_postprocessors,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cap_h/lib/python3.11/site-packages/llama_index/core/response_synthesizers/factory.py:66\u001b[0m, in \u001b[0;36mget_response_synthesizer\u001b[0;34m(llm, prompt_helper, service_context, text_qa_template, refine_template, summary_template, simple_template, response_mode, callback_manager, use_async, streaming, structured_answer_filtering, output_cls, program_factory, verbose)\u001b[0m\n\u001b[1;32m     62\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager \u001b[38;5;129;01mor\u001b[39;00m callback_manager_from_settings_or_context(\n\u001b[1;32m     63\u001b[0m     Settings, service_context\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[0;32m---> 66\u001b[0m prompt_helper \u001b[38;5;241m=\u001b[39m prompt_helper \u001b[38;5;129;01mor\u001b[39;00m prompt_helper_from_settings_or_context(\n\u001b[1;32m     67\u001b[0m     Settings, service_context\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mREFINE:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Refine(\n\u001b[1;32m     72\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     73\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m         service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[1;32m     84\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cap_h/lib/python3.11/site-packages/llama_index/core/settings.py:306\u001b[0m, in \u001b[0;36mprompt_helper_from_settings_or_context\u001b[0;34m(settings, context)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mprompt_helper\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mprompt_helper\n",
      "File \u001b[0;32m~/anaconda3/envs/cap_h/lib/python3.11/site-packages/llama_index/core/settings.py:206\u001b[0m, in \u001b[0;36m_Settings.prompt_helper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the prompt helper.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper \u001b[38;5;241m=\u001b[39m PromptHelper\u001b[38;5;241m.\u001b[39mfrom_llm_metadata(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper \u001b[38;5;241m=\u001b[39m PromptHelper()\n",
      "File \u001b[0;32m~/anaconda3/envs/cap_h/lib/python3.11/site-packages/llama_index/core/indices/prompt_helper.py:117\u001b[0m, in \u001b[0;36mPromptHelper.from_llm_metadata\u001b[0;34m(cls, llm_metadata, chunk_overlap_ratio, chunk_size_limit, tokenizer, separator)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_llm_metadata\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     separator: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPromptHelper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create from llm predictor.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    This will autofill values like context_window and num_output.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     context_window \u001b[38;5;241m=\u001b[39m llm_metadata\u001b[38;5;241m.\u001b[39mcontext_window\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm_metadata\u001b[38;5;241m.\u001b[39mnum_output \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    119\u001b[0m         num_output \u001b[38;5;241m=\u001b[39m DEFAULT_NUM_OUTPUTS\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'context_window'"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22cfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cap_h] *",
   "language": "python",
   "name": "conda-env-cap_h-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
