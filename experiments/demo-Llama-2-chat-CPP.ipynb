{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f05e17",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a510213f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:31:42.132942Z",
     "start_time": "2024-03-06T10:31:37.622295Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests as req\n",
    "import urllib.request\n",
    "\n",
    "import chromadb\n",
    "\n",
    "import openai\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from llama_index.core import Document, Settings, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.nomic import NomicEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from utils_15B import extract_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87579f3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T08:09:52.725379Z",
     "start_time": "2024-03-06T08:09:52.695285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.11\n",
      "IPython version      : 8.22.0\n",
      "\n",
      "llama_index.core: 0.10.12\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p llama_index.core\n",
    "\n",
    "# Python implementation: CPython\n",
    "# Python version       : 3.11.7\n",
    "# IPython version      : 8.20.0\n",
    "\n",
    "# llama_index.core: 0.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc182fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T08:09:52.773333Z",
     "start_time": "2024-03-06T08:09:52.726287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep ^l\n",
    "\n",
    "# llama_cpp_python                         0.2.53\n",
    "# llama-index-core                         0.10.12\n",
    "# llama-index-embeddings-openai            0.1.6\n",
    "# llama-index-llms-llama-cpp               0.1.3\n",
    "# llama-index-llms-openai                  0.1.6\n",
    "# llama-index-vector-stores-chroma         0.0.1\n",
    "# llamaindex-py-client                     0.1.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b27c92",
   "metadata": {},
   "source": [
    "## Verify API tokens are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44ccd20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:31:43.688542Z",
     "start_time": "2024-03-06T10:31:43.681542Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()  # This loads the variables from .envz\n",
    "nomic_api_key = os.getenv(\"NOMIC_API_KEY\")\n",
    "# print(nomic_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9d925",
   "metadata": {},
   "source": [
    "## (Optional) Remove previous JSON files and Chroma DB before starting\n",
    "<span style=\"color: darkred; font-size: 18px;\">using macOS/Linux %%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ./ -type f -name \"*.json\" -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30698a6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c21d4",
   "metadata": {},
   "source": [
    "###  LlamaIndex helper callback functions to add the system prompt and wrap in special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ed1f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:31:49.649697Z",
     "start_time": "2024-03-06T10:31:49.635697Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.base.llms.types import MessageRole\n",
    "\n",
    "BOS, EOS = \"<s>\", \"</s>\"\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def messages_to_prompt_callback(messages, system_prompt=None) -> str:\n",
    "    string_messages: List[str] = []\n",
    "    if messages[0].role == MessageRole.SYSTEM:\n",
    "        # pull out the system message (if it exists in messages)\n",
    "        system_message_str = messages[0].content or \"\"\n",
    "        messages = messages[1:]\n",
    "    else:\n",
    "        system_message_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "    system_message_str = f\"{B_SYS} {system_message_str.strip()} {E_SYS}\"\n",
    "\n",
    "    for i in range(0, len(messages), 2):\n",
    "        # first message should always be a user\n",
    "        user_message = messages[i]\n",
    "        assert user_message.role == MessageRole.USER\n",
    "\n",
    "        if i == 0:\n",
    "            # make sure system prompt is included at the start\n",
    "            str_message = f\"{BOS} {B_INST} {system_message_str} \"\n",
    "        else:\n",
    "            # end previous user-assistant interaction\n",
    "            string_messages[-1] += f\" {EOS}\"\n",
    "            # no need to include system prompt\n",
    "            str_message = f\"{BOS} {B_INST} \"\n",
    "\n",
    "        # include user message content\n",
    "        str_message += f\"{user_message.content} {E_INST}\"\n",
    "\n",
    "        if len(messages) > (i + 1):\n",
    "            # if assistant message exists, add to str_message\n",
    "            assistant_message = messages[i + 1]\n",
    "            assert assistant_message.role == MessageRole.ASSISTANT\n",
    "            str_message += f\" {assistant_message.content}\"\n",
    "\n",
    "        string_messages.append(str_message)\n",
    "\n",
    "    return \"\".join(string_messages)\n",
    "\n",
    "def completion_to_prompt_callback(completion, system_prompt=None) -> str:\n",
    "    system_prompt_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "    return (\n",
    "        f\"{BOS} {B_INST} {B_SYS} {system_prompt_str.strip()} {E_SYS} \"\n",
    "        f\"{completion.strip()} {E_INST}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823c59e",
   "metadata": {},
   "source": [
    "### Llama 2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54387f05",
   "metadata": {},
   "source": [
    "#### Dmitry's CPP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e18e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:31:53.181438Z",
     "start_time": "2024-03-06T10:31:51.991091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\dimson\\AppData\\Local\\llama_index\\models\\llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1950.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1950.00 MiB, K (f16):  975.00 MiB, V (f16):  975.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    16.65 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   275.75 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '2', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "LLAMA2_7B_CHAT = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf\"\n",
    "\n",
    "selected_model = LLAMA2_7B_CHAT\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=selected_model,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=2048,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    messages_to_prompt=messages_to_prompt_callback,\n",
    "    completion_to_prompt=completion_to_prompt_callback\n",
    ")\n",
    "\n",
    "llm.verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c1de1",
   "metadata": {},
   "source": [
    "### using LlamaIndex reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94b50b",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed015a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:31:57.470172Z",
     "start_time": "2024-03-06T10:31:57.456172Z"
    }
   },
   "outputs": [],
   "source": [
    "# using this as baseline standard\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:04.277401Z",
     "start_time": "2024-03-06T10:32:04.260404Z"
    }
   },
   "id": "8060dc57d524fb2b",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "7e8f4eb4",
   "metadata": {},
   "source": [
    "## Fetch data from \"specific\" clinicaltrials.gov\n",
    "<span style=\"color: darkred; font-size: 18px;\"> source: https://drive.google.com/file/d/1HOsN3v8DLzwoMOXOr_Mfb1Hn6XNwlZ72/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b83137c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:10.756639Z",
     "start_time": "2024-03-06T10:32:10.738641Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_trial(nct_id):\n",
    "    trial = req.get(f\"https://clinicaltrials.gov/api/v2/studies/{nct_id}\")\n",
    "    trial_json = trial.json()\n",
    "    return trial_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231c678c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:11.990797Z",
     "start_time": "2024-03-06T10:32:11.982799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some trials to consider (interventional, completed):\n",
    "# nct_id = \"NCT00094887\"\n",
    "# nct_id = \"NCT00108953\"\n",
    "# nct_id = \"NCT00177671\" \n",
    "# nct_id = \"NCT00281918\"\n",
    "# nct_id = \"NCT00404079\"\n",
    "# nct_id = \"NCT00426751\"\n",
    "# nct_id = \"NCT01865747\" #<== good one \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14f90f",
   "metadata": {},
   "source": [
    "### Use just one one trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caef252e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:13.067916Z",
     "start_time": "2024-03-06T10:32:13.056919Z"
    }
   },
   "outputs": [],
   "source": [
    "# list_of_nct_id = [\n",
    "#     \"NCT00094887\",\n",
    "#     \"NCT00108953\",\n",
    "#     \"NCT00177671\",\n",
    "#     \"NCT00281918\",\n",
    "#     \"NCT00404079\",\n",
    "#     \"NCT00426751\",\n",
    "#     \"NCT01865747\",\n",
    "# ]\n",
    "\n",
    "list_of_nct_id = [\n",
    "    \"NCT00108953\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6122ed25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:14.456230Z",
     "start_time": "2024-03-06T10:32:14.241233Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_downloaded_json(list_of_nct_id):\n",
    "    downloaded_json = []\n",
    "    for nct_id in list_of_nct_id:\n",
    "        trial = get_trial(nct_id)\n",
    "        downloaded_json.append(trial)\n",
    "        # save locally for reference\n",
    "        with open(f\"{nct_id}.json\", \"w\") as f:\n",
    "            json.dump(trial, f, indent=4)\n",
    "    return downloaded_json\n",
    "\n",
    "downloaded_json = get_downloaded_json(list_of_nct_id)\n",
    "# downloaded_json[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c098cf2",
   "metadata": {},
   "source": [
    "## Extract a  subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e06d1b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:18.391291Z",
     "start_time": "2024-03-06T10:32:18.382290Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_from_extracted_json(downloaded_json):    \n",
    "    documents_list  = []\n",
    "    for json_file in downloaded_json:\n",
    "        extracted_json = extract_from_json(json_file)\n",
    "        nct_id = json_file['protocolSection']['identificationModule']['nctId']\n",
    "        # save manipulated JSON file to disk for review\n",
    "        save_path = f\"{nct_id}_extracted.json\"\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(extracted_json, f, indent=4)\n",
    "        # prepare for indexing\n",
    "        documents_list.append(extracted_json)\n",
    "    return documents_list\n",
    "    \n",
    "documents_list = list_from_extracted_json(downloaded_json)\n",
    "# len(documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f63e03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:21.746930Z",
     "start_time": "2024-03-06T10:32:21.740930Z"
    }
   },
   "outputs": [],
   "source": [
    "# documents_list[0].keys() # useful later to adjust metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5d6ba",
   "metadata": {},
   "source": [
    "## Llama index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c0507",
   "metadata": {},
   "source": [
    "### add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4005a638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:23.490875Z",
     "start_time": "2024-03-06T10:32:23.482876Z"
    }
   },
   "outputs": [],
   "source": [
    "# all the keys (for metadata)\n",
    "all_keys = list(documents_list[0].keys())\n",
    "# all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c01b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:27.214933Z",
     "start_time": "2024-03-06T10:32:27.195930Z"
    }
   },
   "outputs": [],
   "source": [
    "# to adjust the metadata keys used\n",
    "llm_keys_to_incude = [\n",
    "    \"Brief title\",\n",
    "    \"National Clinical Identification NCT ID\",\n",
    "    \"Lead sponsor\",\n",
    "    \"Arms group 0 intervention names\",\n",
    "    \"Enrollment count\",\n",
    "]\n",
    "\n",
    "llm_keys_to_exclude = [key for key in all_keys if key not in llm_keys_to_incude]\n",
    "\n",
    "# for simplicity, do the same for embedding_keys_to_exclude (in this example)\n",
    "embedding_keys_to_exclude = llm_keys_to_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe96e55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:31.407965Z",
     "start_time": "2024-03-06T10:32:31.394926Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE:  metata data must be one of (str, int, float, None)\n",
    "# use json.dumps() to convert lists and dictionaries into strings\n",
    "\n",
    "def create_llama_docs(documents_list):\n",
    "    llama_documents = []\n",
    "\n",
    "    for trial in documents_list:\n",
    "        trial[\"Brief title\"] = json.dumps(trial[\"Brief title\"])\n",
    "        trial[\"Official title\"] = json.dumps(trial[\"Official title\"])\n",
    "        trial[\"Brief summary\"] = json.dumps(trial[\"Brief summary\"])\n",
    "        trial[\"Detailed description\"] = json.dumps(trial[\"Detailed description\"])\n",
    "        trial[\"Arms group 0 intervention name\"] = json.dumps(trial[\"Arms group 0 intervention name\"])\n",
    "        trial[\"Arms group 1 intervention name\"] = json.dumps(trial[\"Arms group 1 intervention name\"])\n",
    "        trial[\"Eligibility minimum age\"] = json.dumps(trial[\"Eligibility minimum age\"])\n",
    "        trial[\"Organization\"] = json.dumps(trial[\"Organization\"])\n",
    "\n",
    "        # create a Llama Document object \n",
    "        # with text and excluded meta data for llm and embedding model\n",
    "        llama_document = Document(\n",
    "            text=trial[\"Detailed description\"],\n",
    "#             text=json.dumps(trial), #<== testing\n",
    "            metadata=trial,\n",
    "            excluded_llm_metadata_keys=llm_keys_to_exclude,\n",
    "            excluded_embed_metadata_keys=embedding_keys_to_exclude ,\n",
    "            metadata_template=\"{key}=>{value}\",\n",
    "            text_template=\"Metadata:\\n{metadata_str}\\n===========================\\nContent: \\n{content}\"\n",
    "        )\n",
    "        llama_documents.append(llama_document)\n",
    "    \n",
    "    return llama_documents\n",
    "\n",
    "llama_documents = create_llama_docs(documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adf15760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:35.018453Z",
     "start_time": "2024-03-06T10:32:35.009458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      "National Clinical Identification NCT ID=>NCT00108953\n",
      "Brief title=>\"A Research Study to Treat Patients With Advanced Hepatocellular Carcinoma\"\n",
      "Lead sponsor=>Bayer\n",
      "Enrollment count=>96\n",
      "Arms group 0 intervention names=>['Drug: Sorafenib (Nexavar, BAY43-9006) plus Doxorubicin']\n",
      "===========================\n",
      "Content: \n",
      "\"In addition to the key secondary outcome parameters the following parameters will be assessed in an exploratory manner: relative time to progression (TTP), time to symptomatic progression (TTSP), response rate (RR) and overall survival between the 2 study populations.\\n\\nThe possible and potential predictive assays of clinical benefit through an assessment of the correlation between the defined baseline characteristics and key clinical endpoints.\\n\\nThe safety and tolerability will be assessed in the adverse event section. Doxorubicin pharmacokinetics in HCC patients treated with sorafenib versus placebo will be compared and the pharmacokinetic data will be correlated with doxorubicin-related adverse events (i.e., cardiotoxicity).\"\n"
     ]
    }
   ],
   "source": [
    "# Example —LLM sees this:\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6baf3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:36.242247Z",
     "start_time": "2024-03-06T10:32:36.230247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      "National Clinical Identification NCT ID=>NCT00108953\n",
      "Brief title=>\"A Research Study to Treat Patients With Advanced Hepatocellular Carcinoma\"\n",
      "Lead sponsor=>Bayer\n",
      "Enrollment count=>96\n",
      "Arms group 0 intervention names=>['Drug: Sorafenib (Nexavar, BAY43-9006) plus Doxorubicin']\n",
      "===========================\n",
      "Content: \n",
      "\"In addition to the key secondary outcome parameters the following parameters will be assessed in an exploratory manner: relative time to progression (TTP), time to symptomatic progression (TTSP), response rate (RR) and overall survival between the 2 study populations.\\n\\nThe possible and potential predictive assays of clinical benefit through an assessment of the correlation between the defined baseline characteristics and key clinical endpoints.\\n\\nThe safety and tolerability will be assessed in the adverse event section. Doxorubicin pharmacokinetics in HCC patients treated with sorafenib versus placebo will be compared and the pharmacokinetic data will be correlated with doxorubicin-related adverse events (i.e., cardiotoxicity).\"\n"
     ]
    }
   ],
   "source": [
    "# Example — Embedding model sees this:\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5ad18",
   "metadata": {},
   "source": [
    "## Create Nodes\n",
    "<span style=\"color: darkred; font-size: 15px;\">adjust chunk_size, chunk_overlap</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13c22a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:44.945940Z",
     "start_time": "2024-03-06T10:32:44.217433Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_nodes(llama_documents):\n",
    "    parser = SentenceSplitter(chunk_size=2048,chunk_overlap=40) # <== adjust(default = 1024/20)\n",
    "    nodes = parser.get_nodes_from_documents(llama_documents)\n",
    "\n",
    "    for node in nodes:\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=MetadataMode.EMBED)\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    return nodes\n",
    "\n",
    "nodes = create_nodes(llama_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0e579",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c6826f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:55.999702Z",
     "start_time": "2024-03-06T10:32:55.794290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for the CLINICAL_RAG collection in the database...\n",
      "CLINICAL_RAG collection WAS NOT FOUND in Chroma DB, creating...\n",
      "Creating vector store...\n",
      "Creating vector store index\n",
      "record count: 1\n"
     ]
    }
   ],
   "source": [
    "# Chroma DB collection name\n",
    "COLLECTION_NAME = \"CLINICAL_RAG\"\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "print(f\"Looking for the {COLLECTION_NAME} collection in the database...\" )\n",
    "if COLLECTION_NAME not in [col.name for col in db.list_collections()]:\n",
    "    print(f\"{COLLECTION_NAME} collection WAS NOT FOUND in Chroma DB, creating...\")\n",
    "    chroma_collection = db.create_collection(COLLECTION_NAME)\n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    print(\"Creating vector store index\")\n",
    "    index = VectorStoreIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        store_nodes_override=True\n",
    "    )\n",
    "    print(f\"record count: {chroma_collection.count()}\"     \n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(f\"{COLLECTION_NAME} collection WAS FOUND in Chroma DB\")\n",
    "    COLLECTION_NAME = db.get_collection(COLLECTION_NAME)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=COLLECTION_NAME)\n",
    "    print(\"Restoring vector store index from the collection...\")\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model,\n",
    "        store_nodes_override=True\n",
    "    )\n",
    "\n",
    "    print(f\"record count: {COLLECTION_NAME.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fd7f7",
   "metadata": {},
   "source": [
    "### Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e706e81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:32:59.970805Z",
     "start_time": "2024-03-06T10:32:59.903807Z"
    }
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f22cfe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T10:33:47.696175Z",
     "start_time": "2024-03-06T10:33:00.950892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n",
      "\n",
      "llama_print_timings:        load time =    3165.46 ms\n",
      "llama_print_timings:      sample time =      37.75 ms /   242 runs   (    0.16 ms per token,  6409.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3164.95 ms /   485 tokens (    6.53 ms per token,   153.24 tokens per second)\n",
      "llama_print_timings:        eval time =   42591.66 ms /   241 runs   (  176.73 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   46476.68 ms /   726 tokens\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Are there any clinical trials about liver disease?\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d61f6371ec79069e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
